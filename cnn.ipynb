{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the notebook where I will augment my data extract my Features,and decide what architecture to use and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import warnings\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import librosa\n",
    "from torchaudio.functional import preemphasis\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\CNN Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>chapter</th>\n",
       "      <th>reciter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abd_albassit_abd_samad_1_rir.wav</td>\n",
       "      <td>surah_al_dhoha</td>\n",
       "      <td>abd_albassit_abd_samad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abd_albassit_abd_samad_1_org.wav</td>\n",
       "      <td>surah_al_dhoha</td>\n",
       "      <td>abd_albassit_abd_samad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abd_albassit_abd_samad_2_bg.wav</td>\n",
       "      <td>surah_al_dhoha</td>\n",
       "      <td>abd_albassit_abd_samad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abd_albassit_abd_samad_2_org.wav</td>\n",
       "      <td>surah_al_dhoha</td>\n",
       "      <td>abd_albassit_abd_samad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abd_albassit_abd_samad_3_bg.wav</td>\n",
       "      <td>surah_al_dhoha</td>\n",
       "      <td>abd_albassit_abd_samad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path         chapter                 reciter\n",
       "0  abd_albassit_abd_samad_1_rir.wav  surah_al_dhoha  abd_albassit_abd_samad\n",
       "1  abd_albassit_abd_samad_1_org.wav  surah_al_dhoha  abd_albassit_abd_samad\n",
       "2   abd_albassit_abd_samad_2_bg.wav  surah_al_dhoha  abd_albassit_abd_samad\n",
       "3  abd_albassit_abd_samad_2_org.wav  surah_al_dhoha  abd_albassit_abd_samad\n",
       "4   abd_albassit_abd_samad_3_bg.wav  surah_al_dhoha  abd_albassit_abd_samad"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"dataset2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my possible options:-Noise Injection/-Reverberation /- Background Noise Simulation/-Volume Perturbation/-Clipping/-Equalization (EQ) Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading Data Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, transform=None, max_length=22050*5,is_test=False):#max length change \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        if not is_test:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.data['reciter'] = self.label_encoder.fit_transform(self.data['reciter'])\n",
    "        self.is_test = is_test\n",
    "\n",
    "        # Pre-load all audio files\n",
    "        self.audio_data = self.preload_audio(self.max_length)\n",
    "\n",
    "    def preload_audio(self,max_length):\n",
    "        audio_data = {}\n",
    "        for _, row in tqdm(self.data.iterrows(), total=len(self.data), desc=\"Preloading audio\"):\n",
    "            audio_path = os.path.join(self.audio_dir, row['path'])\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            if sample_rate != 16000:\n",
    "                # Resample the audio\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            # if waveform.shape[1] < self.max_length:\n",
    "            #     padding = torch.zeros(1, self.max_length - waveform.shape[1])\n",
    "            #     waveform = torch.cat([waveform, padding], dim=1)\n",
    "            # else:\n",
    "            #     waveform = waveform[:, :self.max_length]\n",
    "            if waveform.shape[-1] < max_length:\n",
    "                padding = torch.zeros(max_length - waveform.shape[-1])\n",
    "                padded_audio = torch.cat((waveform, padding), dim=-1)\n",
    "                audio_data[row['path']] = padded_audio\n",
    "            elif waveform.shape[-1] > max_length:\n",
    "                truncated_audio = waveform[:, :max_length]\n",
    "                audio_data[row['path']] = truncated_audio\n",
    "            else:\n",
    "                audio_data[row['path']] = waveform\n",
    "        return audio_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audio_filepath = self.data.iloc[idx]['path']\n",
    "        waveform = self.audio_data[audio_filepath]\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        \n",
    "        if self.is_test:\n",
    "            return waveform\n",
    "        else:\n",
    "            label = self.data.iloc[idx]['reciter']\n",
    "            return waveform, label\n",
    "       \n",
    "\n",
    "def optimized_audio_transform(waveform, sample_rate=16000, n_mfcc=30, n_fft=400, hop_length=120):\n",
    "    # Compute MFCC\n",
    "    mfcc_transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\n",
    "            \"n_fft\": n_fft,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"n_mels\": 128,\n",
    "            \"center\": False\n",
    "        }\n",
    "    )\n",
    "    mfcc = mfcc_transform(waveform)  # Shape: (batch, n_mfcc, timesteps)\n",
    "\n",
    "\n",
    "    # Reshape to (timesteps, features)\n",
    "    features = mfcc.squeeze(0).transpose(0, 1)  # Shape: (timesteps, features)\n",
    "\n",
    "    return features # Reshape to (time_steps, n_lfcc)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    if isinstance(batch[0], tuple):\n",
    "        waveforms, labels = zip(*batch)\n",
    "        waveforms = torch.stack(waveforms)\n",
    "        return waveforms, torch.tensor(labels)\n",
    "    else:\n",
    "        return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading audio: 100%|██████████| 6650/6650 [01:18<00:00, 84.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = OptimizedAudioDataset(csv_file=\"dataset2.csv\",\n",
    "                                      audio_dir=\"Dataset\",\n",
    "                                      transform=optimized_audio_transform)\n",
    "\n",
    "train_data, val_data = train_test_split(train_dataset, test_size=0.1, random_state=42,\n",
    "                                        stratify=train_dataset.data['reciter'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True,\n",
    "                          collate_fn=custom_collate_fn, num_workers=0)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=100, shuffle=False,\n",
    "                        collate_fn=custom_collate_fn, num_workers=0)\n",
    "\n",
    "# test_dataset = OptimizedAudioDataset(csv_file=\"/content/Test_1 (1).csv\",\n",
    "#                                      audio_dir=\"/content/drive/MyDrive/Cleaned_TechCabal_01\",\n",
    "#                                      transform=optimized_audio_transform,\n",
    "#                                      is_test=True)\n",
    "\n",
    "# test_loader = DataLoader(test_dataset, batch_size=70, shuffle=False,\n",
    "#                          collate_fn=custom_collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for i in range (1000):\n",
    "    data,label=train_data[i]\n",
    "    t.append(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({690: 1000})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SOrthConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, padding_mode='zeros'):\n",
    "        '''\n",
    "        Conv1d with a method for stepping towards semi-orthongonality\n",
    "        http://danielpovey.com/files/2018_interspeech_tdnnf.pdf\n",
    "        '''\n",
    "        super(SOrthConv, self).__init__()\n",
    "\n",
    "        kwargs = {'bias': False}\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels,\n",
    "                              kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation,\n",
    "                              bias=False, padding_mode=padding_mode)\n",
    "       \n",
    "        self.reset_parameters()\n",
    "        #alternative for initializeing parameters \n",
    "        # nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def step_semi_orth(self):\n",
    "        with torch.no_grad():\n",
    "            M = self.get_semi_orth_weight(self.conv)\n",
    "            self.conv.weight.copy_(M)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Standard dev of M init values is inverse of sqrt of num cols\n",
    "        # Glorot-style initialization\n",
    "        nn.init._no_grad_normal_(self.conv.weight, 0.,\n",
    "                                 self.get_M_shape(self.conv.weight)[1]**-0.5)\n",
    "\n",
    "    def orth_error(self):\n",
    "        return self.get_semi_orth_error(self.conv).item()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_semi_orth_weight(conv1dlayer):\n",
    "        # updates conv1 weight M using update rule to make it more semi orthogonal\n",
    "        # based off ConstrainOrthonormalInternal in nnet-utils.cc in Kaldi src/nnet3\n",
    "        # includes the tweaks related to slowing the update speed\n",
    "        # only an implementation of the 'floating scale' case\n",
    "        with torch.no_grad():\n",
    "            update_speed = 0.125 #1/8\n",
    "            orig_shape = conv1dlayer.weight.shape\n",
    "            # a conv weight differs slightly from TDNN formulation:\n",
    "            # Conv weight: (out_filters, in_filters, kernel_width)\n",
    "            # TDNN weight M is of shape: (in_dim, out_dim) or [rows, cols]\n",
    "            # the in_dim of the TDNN weight is equivalent to in_filters * kernel_width of the Conv\n",
    "            M = conv1dlayer.weight.reshape(\n",
    "                orig_shape[0], orig_shape[1]*orig_shape[2]).T\n",
    "            # M now has shape (in_dim[rows], out_dim[cols])\n",
    "            mshape = M.shape\n",
    "            if mshape[0] > mshape[1]:  # semi orthogonal constraint for rows > cols\n",
    "                M = M.T\n",
    "            P = torch.mm(M, M.T)\n",
    "            PP = torch.mm(P, P.T)\n",
    "            trace_P = torch.trace(P) #trace_P is the sum of eigenvalues of P\n",
    "            trace_PP = torch.trace(PP)#trace_P_P is the sum-square of eigenvalues of P (f)\n",
    "            ratio = trace_PP * P.shape[0] / (trace_P * trace_P)\n",
    "\n",
    "            # the following is the tweak to avoid divergence (more info in Kaldi)\n",
    "            assert ratio > 0.99\n",
    "            if ratio > 1.02:\n",
    "                update_speed *= 0.5\n",
    "                if ratio > 1.1:\n",
    "                    update_speed *= 0.5\n",
    "\n",
    "            scale2 = trace_PP/trace_P\n",
    "            update = P - (torch.matrix_power(P, 0) * scale2)\n",
    "            alpha = update_speed / scale2\n",
    "            update = (-4.0 * alpha) * torch.mm(update, M)\n",
    "            updated = M + update\n",
    "            # updated has shape (cols, rows) if rows > cols, else has shape (rows, cols)\n",
    "            # Transpose (or not) to shape (cols, rows) (IMPORTANT, s.t. correct dimensions are reshaped)\n",
    "            # Then reshape to (cols, in_filters, kernel_width)\n",
    "            return updated.reshape(*orig_shape) if mshape[0] > mshape[1] else updated.T.reshape(*orig_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_M_shape(conv_weight):\n",
    "        orig_shape = conv_weight.shape #(out_channels, in_channels, kernel_size)\n",
    "        return (orig_shape[1]*orig_shape[2], orig_shape[0]) #Transformed shape (M) = (64 × 3, 128) → (192, 128)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_semi_orth_error(conv1dlayer):\n",
    "        with torch.no_grad():\n",
    "            orig_shape = conv1dlayer.weight.shape\n",
    "            M = conv1dlayer.weight.reshape(\n",
    "                orig_shape[0], orig_shape[1]*orig_shape[2]).T\n",
    "            mshape = M.shape\n",
    "            if mshape[0] > mshape[1]:  # semi orthogonal constraint for rows > cols\n",
    "                M = M.T\n",
    "            P = torch.mm(M, M.T)\n",
    "            PP = torch.mm(P, P.T)\n",
    "            trace_P = torch.trace(P)\n",
    "            trace_PP = torch.trace(PP)\n",
    "            scale2 = torch.sqrt(trace_PP/trace_P) ** 2\n",
    "            update = P - (torch.matrix_power(P, 0) * scale2)\n",
    "            return torch.norm(update, p='fro')\n",
    "\n",
    "#Continuous scaled dropout\n",
    "class SharedDimScaleDropout(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.5, dim=1):\n",
    "        '''\n",
    "        Continuous scaled dropout that is const over chosen dim (usually across time)\n",
    "        Multiplies inputs by random mask taken from Uniform([1 - 2\\alpha, 1 + 2\\alpha])\n",
    "\n",
    "        For time-series or sequence models (e.g., RNNs, Transformers), the mask is constant along the time dimension, preserving temporal structure.\n",
    "        '''\n",
    "        super(SharedDimScaleDropout, self).__init__()\n",
    "        if alpha > 0.5 or alpha < 0:\n",
    "            raise ValueError(\"alpha must be between 0 and 0.5\")\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.register_buffer('mask', torch.tensor(0.)) #method used to register a tensor as part of the model without considering it as a trainable parameter\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.training:\n",
    "            if self.alpha != 0.:\n",
    "                # sample mask from uniform dist with dim of length 1 in self.dim and then repeat to match size\n",
    "                tied_mask_shape = list(X.shape)\n",
    "                tied_mask_shape[self.dim] = 1\n",
    "                repeats = [1 if i != self.dim else X.shape[self.dim]\n",
    "                           for i in range(len(X.shape))]\n",
    "                return X * self.mask.repeat(tied_mask_shape).uniform_(1 - 2*self.alpha, 1 + 2*self.alpha).repeat(repeats)\n",
    "                # expected value of dropout mask is 1 so no need to scale outputs like vanilla dropout\n",
    "        return X\n",
    "\n",
    "#Cristal CLear\n",
    "class FTDNNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bottleneck_dim, context_size=2, dilations=None, paddings=None, alpha=0.0):\n",
    "        '''\n",
    "        3 stage factorised TDNN http://danielpovey.com/files/2018_interspeech_tdnnf.pdf\n",
    "        '''\n",
    "        \"\"\"Yes, this code implements the concept of factorizing the convolution as described in the paper.\n",
    "          It breaks down a potentially large convolution (like a 3x1 convolution) into two smaller convolutions (2x1),\n",
    "            with the first two layers being constrained by the semi-orthogonal property. The third layer is a regular convolution\n",
    "            that maps to the final output dimension.\n",
    "            {The setup described above uses a constrained\n",
    "            3x1 convolution followed by 1x1 convolution (or equivalently, a TDNN\n",
    "            layer splicing 3 frames followed immediately by a feedforward\n",
    "            layer). We have found that better results can be obtained by\n",
    "            using a constrained 2x1 convolution followed by a 2x1 convolution. We refer to this as “factorizing the convolution”.}\n",
    "\n",
    "            However here we use 3 stage sliceing :3-stage splicing\n",
    "            {Something that we have found to be even better than the “factorized convolution” above, is to have a layer with a constrained\n",
    "            2x1 convolution to dimension 256, followed by another constrained 2x1 convolution to dimension 256, followed by a 2x1\n",
    "            convolution back to the hidden-layer dimension (e.g. 1280).\n",
    "            The dimension now goes from, for example, 1280 → 256 →\n",
    "            256 → 1280, within one layer. The effective temporal context\n",
    "            of this setup is of course wider than the TDNN baseline, due to\n",
    "            the extra 2x1 convolution}\n",
    "\"\"\"\n",
    "        super(FTDNNLayer, self).__init__()\n",
    "        paddings = [1, 1, 1] if not paddings else paddings\n",
    "        dilations = [2, 2, 2] if not dilations else dilations\n",
    "        assert len(paddings) == 3\n",
    "        assert len(dilations) == 3\n",
    "        self.factor1 = SOrthConv(in_dim, bottleneck_dim, context_size, \n",
    "                                 padding=paddings[0], dilation=dilations[0])\n",
    "        self.factor2 = SOrthConv(bottleneck_dim, bottleneck_dim,\n",
    "                                 context_size, padding=paddings[1], dilation=dilations[1]) #we used Bottelneck to reduce feature space and improve effeciency ==> Why use it? → Fewer parameters, better generalization, faster inference.\n",
    "        self.factor3 = nn.Conv1d(bottleneck_dim, out_dim, context_size,\n",
    "                                 padding=paddings[2], dilation=dilations[2], bias=False)\n",
    "        self.nl = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = SharedDimScaleDropout(alpha=alpha, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' input (batch_size, seq_len, in_dim) '''\n",
    "        assert (x.shape[-1] == self.factor1.conv.weight.shape[1])\n",
    "        x = self.factor1(x.transpose(1, 2))\n",
    "        x = self.factor2(x)\n",
    "        x = self.factor3(x)\n",
    "        x = self.nl(x)\n",
    "        x = self.bn(x).transpose(1, 2) #batchNorm1 expects a  (batch_size, num_channels, seq_len) format\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def step_semi_orth(self):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, SOrthConv):\n",
    "                layer.step_semi_orth()\n",
    "\n",
    "    def orth_error(self):\n",
    "        orth_error = 0\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, SOrthConv):\n",
    "                orth_error += layer.orth_error()\n",
    "        return orth_error\n",
    "\n",
    "#Clearr\n",
    "class DenseReLU(nn.Module): \n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DenseReLU, self).__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.nl = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x) #expects (batch_size, sequence_length, in_dim).\n",
    "        x = self.nl(x)\n",
    "        if len(x.shape) > 2:\n",
    "            x = self.bn(x.transpose(1, 2)).transpose(1, 2) #expects input of shape (batch_size, features, sequence_length)\n",
    "        else:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "#meth1 :simple pooling layer [CLear] \n",
    "class StatsPool(nn.Module):\n",
    "\n",
    "    def __init__(self, floor=1e-10, bessel=False):\n",
    "        super(StatsPool, self).__init__()\n",
    "        self.floor = floor\n",
    "        self.bessel = bessel\n",
    "\n",
    "    def forward(self, x):\n",
    "        #(batch_size, sequence_length, feature_dim)\n",
    "        means = torch.mean(x, dim=1)\n",
    "        _, t, _ = x.shape\n",
    "        if self.bessel:\n",
    "            t = t - 1\n",
    "        residuals = x - means.unsqueeze(1)\n",
    "        numerator = torch.sum(residuals**2, dim=1)\n",
    "        stds = torch.sqrt(torch.clamp(numerator, min=self.floor)/t)\n",
    "        x = torch.cat([means, stds], dim=1) #[batch,means*stds]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#meth2 : temporal pooling functions with learnable parameters:NEtVlad [CLearrrr]\n",
    "class NetVLAD(nn.Module):\n",
    "    def __init__(self, num_clusters=8, dim=128, alpha=100.0, normalize_input=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_clusters (int): Number of clusters (K).\n",
    "            dim (int): Dimension of the input descriptors.\n",
    "            alpha (float): Scaling parameter used to sharpen or smooth the soft assignment.\n",
    "            normalize_input (bool): If True, L2 normalize input descriptors along the feature dimension.\n",
    "        \"\"\"\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.normalize_input = normalize_input\n",
    "\n",
    "        # This convolution layer computes the assignment weights for each descriptor to each cluster.\n",
    "        # It takes input of size [B, dim, N] and outputs a score for each cluster at each time step.\n",
    "        self.conv = nn.Conv1d(dim, num_clusters, kernel_size=1, bias=True)\n",
    "        \n",
    "        # The cluster centers (or \"visual words\"). This is a trainable parameter of shape [K, dim].\n",
    "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, dim, N], where:\n",
    "                B = batch size,\n",
    "                dim = feature dimension,\n",
    "                N = number of frames (or audio descriptors).\n",
    "                \n",
    "        Returns:\n",
    "            torch.Tensor: The NetVLAD descriptor for each input in the batch.\n",
    "                          Output shape is [B, num_clusters * dim].\n",
    "        \"\"\"\n",
    "        # Optionally normalize each descriptor (feature vector) to unit length.\n",
    "        if self.normalize_input:\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        # Compute the assignment scores using the 1x1 convolution.\n",
    "        # Output shape: [B, num_clusters, N]\n",
    "        soft_assign = self.conv(x)\n",
    "        \n",
    "        # Apply the softmax along the cluster dimension to get soft assignments.\n",
    "        # We scale the scores with alpha to control the sharpness.\n",
    "        soft_assign = F.softmax(self.alpha * soft_assign, dim=1)\n",
    "\n",
    "        # Permute x to shape [B, N, dim] for easier computation of residuals.\n",
    "        x_flatten = x.permute(0, 2, 1).contiguous()  # Shape: [B, N, dim]\n",
    "\n",
    "        # Expand the cluster centers to match the batch size.\n",
    "        # New shape: [B, num_clusters, dim]\n",
    "        centroids = self.centroids.expand(x.size(0), -1, -1)\n",
    "\n",
    "        # Compute the residuals between each descriptor and each cluster center.\n",
    "        # First, expand x_flatten to shape [B, 1, N, dim]\n",
    "        # Expand centroids to shape [B, num_clusters, 1, dim]\n",
    "        x_expanded = x_flatten.unsqueeze(1)         # Shape: [B, 1, N, dim]\n",
    "        centroids_expanded = centroids.unsqueeze(2)   # Shape: [B, num_clusters, 1, dim]\n",
    "        residuals = x_expanded - centroids_expanded   # Shape: [B, num_clusters, N, dim]\n",
    "        # Weight the residuals by the soft assignments.\n",
    "        # soft_assign is [B, num_clusters, N] so unsqueeze the last dimension.\n",
    "        soft_assign = soft_assign.unsqueeze(-1)      # Shape: [B, num_clusters, N, 1]\n",
    "        weighted_residuals = soft_assign * residuals   # Shape: [B, num_clusters, N, dim]\n",
    "\n",
    "        # Aggregate (sum) the weighted residuals over all descriptors (N).\n",
    "        vlad = weighted_residuals.sum(dim=2)           # Shape: [B, num_clusters, dim]\n",
    "\n",
    "        # Intra-normalization: normalize each cluster’s descriptor vector.\n",
    "        vlad = F.normalize(vlad, p=2, dim=2)\n",
    "\n",
    "        # Flatten the VLAD descriptor into a single vector per sample.\n",
    "        vlad = vlad.view(x.size(0), -1)                # Shape: [B, num_clusters * dim]\n",
    "\n",
    "        # L2 normalize the final VLAD vector.\n",
    "        vlad = F.normalize(vlad, p=2, dim=1)\n",
    "\n",
    "        return vlad\n",
    "\n",
    "#Clearr\n",
    "class AAMSoftmax(nn.Module): \n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): Dimension of the input speaker embedding.\n",
    "            out_features (int): Number of speaker classes.\n",
    "            s (float): Scale factor for the logits (commonly set to a value like 30.0).\n",
    "            m (float): Angular margin (in radians) to be added to the target class.\n",
    "        \"\"\"\n",
    "        super(AAMSoftmax, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s  # scaling factor\n",
    "        self.m = m  # angular margin\n",
    "\n",
    "        # Weight matrix for classification, shape: [num_classes, in_features].\n",
    "        # This acts as the class centers (or prototypes).\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embedding of shape [batch_size, in_features].\n",
    "            labels (torch.LongTensor): Ground truth labels of shape [batch_size].\n",
    "        Returns:\n",
    "            torch.Tensor: Scaled logits with the angular margin applied to the target classes.\n",
    "        \"\"\"\n",
    "        # Normalize the speaker embedding vectors and the weight vectors\n",
    "        x_norm = F.normalize(x, p=2, dim=1)\n",
    "        weight_norm = F.normalize(self.weight, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity between embeddings and class centers.\n",
    "        # Resulting shape: [batch_size, num_classes]\n",
    "        cosine = F.linear(x_norm, weight_norm)\n",
    "\n",
    "        # Clamp cosine values to the valid range of arccos, i.e., [-1, 1], to ensure numerical stability.\n",
    "        cosine = cosine.clamp(-1, 1)\n",
    "        # Compute the angle (theta) between x and each class center.\n",
    "        theta = torch.acos(cosine)  \n",
    "\n",
    "        # For the target classes, add the angular margin m.\n",
    "        # This effectively shifts the angle for the correct class by m radians.\n",
    "        theta_margin = theta + self.m\n",
    "        # Compute cos(theta + m) using the cosine addition formula:\n",
    "        # cos(theta + m) = cos(theta)*cos(m) - sin(theta)*sin(m)\n",
    "        cosine_margin = torch.cos(theta_margin)\n",
    "\n",
    "        # Create one-hot encoding for the labels.\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
    "\n",
    "        # For target classes, replace cosine with cosine_margin.\n",
    "        # For non-target classes, keep the original cosine.\n",
    "        logits = self.s * (one_hot * cosine_margin + (1.0 - one_hot) * cosine)\n",
    "\n",
    "        return logits\n",
    "\n",
    "#CLear as a whistle tdnn simple layer\n",
    "class TDNN(nn.Module): \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=23,\n",
    "        output_dim=512,\n",
    "        context_size=5,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        batch_norm=True,\n",
    "        dropout_p=0.0,\n",
    "        padding=0\n",
    "    ):\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv = nn.Conv1d(self.input_dim,\n",
    "                                self.output_dim,\n",
    "                                self.context_size,\n",
    "                                stride=self.stride,\n",
    "                                padding=self.padding,\n",
    "                                dilation=self.dilation)\n",
    "\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        self.drop = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        outpu: size (batch, new_seq_len, output_features)\n",
    "        '''\n",
    "\n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(\n",
    "            self.input_dim, d)\n",
    "\n",
    "        x = self.conv(x.transpose(1, 2)) # (batch_size, input_dim, sequence_length)\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        return x.transpose(1, 2) # Back to (batch_size, sequence_length, output_dim)\n",
    "\n",
    "#CLear as a whistle just need to check the last methods \n",
    "class XEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim=30):\n",
    "        '''\n",
    "        The FTDNN architecture from\n",
    "        \"State-of-the-art speaker recognition with neural network embeddings in \n",
    "        NIST SRE18 and Speakers in the Wild evaluations\"\n",
    "        https://www.sciencedirect.com/science/article/pii/S0885230819302700\n",
    "        '''\n",
    "        super(XEmbedding, self).__init__()\n",
    "\n",
    "        self.layer01 = TDNN(input_dim=in_dim, output_dim=512,\n",
    "                            context_size=5, padding=2)\n",
    "        self.layer02 = FTDNNLayer(512, 1024, 256, context_size=2, dilations=[\n",
    "                                  2, 2, 2], paddings=[1, 1, 1])\n",
    "        self.layer03 = FTDNNLayer(1024, 1024, 256, context_size=1, dilations=[\n",
    "                                  1, 1, 1], paddings=[0, 0, 0])\n",
    "        self.layer04 = FTDNNLayer(1024, 1024, 256, context_size=2, dilations=[\n",
    "                                  3, 3, 2], paddings=[2, 1, 1])\n",
    "        self.layer05 = FTDNNLayer(2048, 1024, 256, context_size=1, dilations=[\n",
    "                                  1, 1, 1], paddings=[0, 0, 0])\n",
    "        self.layer06 = FTDNNLayer(1024, 1024, 256, context_size=2, dilations=[\n",
    "                                  3, 3, 2], paddings=[2, 1, 1])\n",
    "        self.layer07 = FTDNNLayer(3072, 1024, 256, context_size=2, dilations=[\n",
    "                                  3, 3, 2], paddings=[2, 1, 1])\n",
    "        self.layer08 = FTDNNLayer(1024, 1024, 256, context_size=2, dilations=[\n",
    "                                  3, 3, 2], paddings=[2, 1, 1])\n",
    "        self.layer09 = FTDNNLayer(3072, 1024, 256, context_size=1, dilations=[\n",
    "                                  1, 1, 1], paddings=[0, 0, 0])\n",
    "        self.layer10 = DenseReLU(1024, 2048)\n",
    "\n",
    "        self.layer11 = StatsPool()\n",
    "\n",
    "        self.layer12 = DenseReLU(4096, 512)\n",
    "\n",
    "        self.layer13=AAMSoftmax(in_features=512, out_features=17, s=30.0, m=0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x,labels=None):\n",
    "        '''\n",
    "        Input must be (batch_size, seq_len, in_dim)\n",
    "        '''\n",
    "        x = self.layer01(x)\n",
    "        x_2 = self.layer02(x)\n",
    "        x_3 = self.layer03(x_2)\n",
    "        x_4 = self.layer04(x_3)\n",
    "        skip_5 = torch.cat([x_4, x_3], dim=-1)\n",
    "        x = self.layer05(skip_5)\n",
    "        x_6 = self.layer06(x)\n",
    "        skip_7 = torch.cat([x_6, x_4, x_2], dim=-1)\n",
    "        x = self.layer07(skip_7)\n",
    "        x_8 = self.layer08(x)\n",
    "        skip_9 = torch.cat([x_8, x_6, x_4], dim=-1)\n",
    "        x = self.layer09(skip_9)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        if labels is not None:\n",
    "            logits = self.layer13(x, labels)\n",
    "            return logits\n",
    "        else:\n",
    "        # During inference, compute logits and return predicted class\n",
    "            logits = self.layer13(x, torch.zeros(x.size(0), dtype=torch.long, device=x.device))\n",
    "            probabilities = F.softmax(logits, dim=1)  # Shape: [batch_size, num_classes]\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)  # Shape: [batch_size]\n",
    "            return predicted_class\n",
    "        # return x\n",
    "\n",
    "    def step_ftdnn_layers(self):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, FTDNNLayer):\n",
    "                layer.step_semi_orth()\n",
    "\n",
    "    def set_dropout_alpha(self, alpha):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, FTDNNLayer):\n",
    "                layer.dropout.alpha = alpha\n",
    "\n",
    "    def get_orth_errors(self):\n",
    "        errors = 0.\n",
    "        with torch.no_grad():\n",
    "            for layer in self.children():\n",
    "                if isinstance(layer, FTDNNLayer):\n",
    "                    errors += layer.orth_error()\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdnn_f = FTDNNLayer(1280, 512, 256, context_size=2, dilations=[2,2,2], paddings=[1,1,1])\n",
    "# This is a sequence of three 2x1 convolutions\n",
    "# dimensions go from 1280 -> 256 -> 256 -> 512\n",
    "# dilations and paddings handles how much to dilate and pad each convolution\n",
    "# Having these configurable is to ensure the sequence length stays the same\n",
    "\n",
    "test_input = torch.rand(5, 100, 1280)\n",
    "# inputs to the FTDNNLayer must be (batch_size, seq_len, in_dim)\n",
    "\n",
    "tdnn_f(test_input).shape # returns (5, 100, 512)\n",
    "\n",
    "tdnn_f.step_semi_orth() # The key method to constrain the first two convolutions, perform after every SGD step\n",
    "\n",
    "tdnn_f.orth_error() # This returns the orth error of the constrained convs, useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "void ConstrainOrthonormalInternal(BaseFloat scale,\n",
    "                                  const std::string &component_name,\n",
    "                                  CuMatrixBase<BaseFloat> *M) {\n",
    "  KALDI_ASSERT(scale != 0.0);\n",
    "\n",
    "  // We'd like to enforce the rows of M to be orthonormal.\n",
    "  // define P = M M^T.  If P is unit then M has orthonormal rows.\n",
    "  // We actually want P to equal scale^2 * I, so that M's rows are\n",
    "  // orthogonal with 2-norms equal to 'scale'.\n",
    "  // We (notionally) add to the objective function, the value\n",
    "  // -alpha times the sum of squared elements of Q = (P - scale^2 * I).\n",
    "  int32 rows = M->NumRows(), cols = M->NumCols();\n",
    "  CuMatrix<BaseFloat> M_update(rows, cols);\n",
    "  CuMatrix<BaseFloat> P(rows, rows);\n",
    "  P.SymAddMat2(1.0, *M, kNoTrans, 0.0);\n",
    "  P.CopyLowerToUpper();\n",
    "\n",
    "  // The 'update_speed' is a constant that determines how fast we approach a\n",
    "  // matrix with the desired properties (larger -> faster).  Larger values will\n",
    "  // update faster but will be more prone to instability.  0.125 (1/8) is the\n",
    "  // value that gives us the fastest possible convergence when we are already\n",
    "  // close to be a semi-orthogonal matrix (in fact, it will lead to quadratic\n",
    "  // convergence).\n",
    "  // See  http://www.danielpovey.com/files/2018_interspeech_tdnnf.pdf\n",
    "  // for more details.\n",
    "  BaseFloat update_speed = 0.125;\n",
    "  bool floating_scale = (scale < 0.0);\n",
    "\n",
    "\n",
    "  if (floating_scale) {\n",
    "    // This (letting the scale \"float\") is described in Sec. 2.3 of\n",
    "    // http://www.danielpovey.com/files/2018_interspeech_tdnnf.pdf,\n",
    "    // where 'scale' here is written 'alpha' in the paper.\n",
    "    //\n",
    "    // We pick the scale that will give us an update to M that is\n",
    "    // orthogonal to M (viewed as a vector): i.e., if we're doing\n",
    "    // an update M := M + X, then we want to have tr(M X^T) == 0.\n",
    "    // The following formula is what gives us that.\n",
    "    // With P = M M^T, our update formula is doing to be:\n",
    "    //  M := M + (-4 * alpha * (P - scale^2 I) * M).\n",
    "    // (The math below explains this update formula; for now, it's\n",
    "    // best to view it as an established fact).\n",
    "    // So X (the change in M) is -4 * alpha * (P - scale^2 I) * M,\n",
    "    // where alpha == update_speed / scale^2.\n",
    "    // We want tr(M X^T) == 0.  First, forget the -4*alpha, because\n",
    "    // we don't care about constant factors.  So we want:\n",
    "    //  tr(M * M^T * (P - scale^2 I)) == 0.\n",
    "    // Since M M^T == P, that means:\n",
    "    //  tr(P^2 - scale^2 P) == 0,\n",
    "    // or scale^2 = tr(P^2) / tr(P).\n",
    "    // Note: P is symmetric so it doesn't matter whether we use tr(P P) or\n",
    "    // tr(P^T P); we use tr(P^T P) because I believe it's faster to compute.\n",
    "\n",
    "    BaseFloat trace_P = P.Trace(), trace_P_P = TraceMatMat(P, P, kTrans);\n",
    "\n",
    "    if (trace_P < 1.0e-15)\n",
    "      return;   // This matrix has almost zero value.  It can happen when\n",
    "                // components are unused.\n",
    "\n",
    "    scale = std::sqrt(trace_P_P / trace_P);\n",
    "\n",
    "    // The following is a tweak to avoid divergence when the eigenvalues aren't\n",
    "    // close to being the same.  trace_P is the sum of eigenvalues of P, and\n",
    "    // trace_P_P is the sum-square of eigenvalues of P.  Treat trace_P as a sum\n",
    "    // of positive values, and trace_P_P as their sumsq.  Then mean = trace_P /\n",
    "    // dim, and trace_P_P cannot be less than dim * (trace_P / dim)^2,\n",
    "    // i.e. trace_P_P >= trace_P^2 / dim.  If ratio = trace_P_P * dim /\n",
    "    // trace_P^2, then ratio >= 1.0, and the excess above 1.0 is a measure of\n",
    "    // how far we are from convergence.  If we're far from convergence, we make\n",
    "    // the learning rate slower to reduce the risk of divergence, since the\n",
    "    // update may not be stable for starting points far from equilibrium.\n",
    "    BaseFloat ratio = (trace_P_P * P.NumRows() / (trace_P * trace_P));\n",
    "    if (!(ratio > 0.99)) {\n",
    "      KALDI_WARN << \"Ratio is \" << ratio << \" (should be >= 1.0); component is \"\n",
    "                 << component_name;\n",
    "      KALDI_ASSERT(ratio > 0.9);\n",
    "    }\n",
    "    if (ratio > 1.02) {\n",
    "      KALDI_WARN << \"Ratio is \" << ratio << \", multiplying update speed \"\n",
    "                 << \"(currently \" << update_speed << \") by 0.5; component is \"\n",
    "                 << component_name;\n",
    "      update_speed *= 0.5;  // Slow down the update speed to reduce the risk of divergence.\n",
    "      if (ratio > 1.1) update_speed *= 0.5;  // Slow it down even more.\n",
    "    }\n",
    "  }\n",
    "\n",
    "  P.AddToDiag(-1.0 * scale * scale);\n",
    "\n",
    "  // We may want to un-comment the following code block later on if we have a\n",
    "  // problem with instability in setups with a non-floating orthonormal\n",
    "  // constraint.\n",
    "  /*\n",
    "  if (!floating_scale) {\n",
    "    // This is analogous to the stuff with 'ratio' above, but when we don't have\n",
    "    // a floating scale.  It reduces the chances of divergence when we have\n",
    "    // a bad initialization.\n",
    "    BaseFloat error = P.FrobeniusNorm(),\n",
    "        error_proportion = error * error / P.NumRows();\n",
    "    // 'error_proportion' is the sumsq of elements in (P - I) divided by the\n",
    "    // sumsq of elements of I.  It should be much less than one (i.e. close to\n",
    "    // zero) if the error is small.\n",
    "    if (error_proportion > 0.02) {\n",
    "      update_speed *= 0.5;\n",
    "      if (error_proportion > 0.1)\n",
    "        update_speed *= 0.5;\n",
    "    }\n",
    "  }\n",
    "  */\n",
    "\n",
    "  if (GetVerboseLevel() >= 1) {\n",
    "    BaseFloat error = P.FrobeniusNorm();\n",
    "    KALDI_VLOG(2) << \"Error in orthogonality is \" << error;\n",
    "  }\n",
    "\n",
    "  // see Sec. 2.2 of http://www.danielpovey.com/files/2018_interspeech_tdnnf.pdf\n",
    "  // for explanation of the 1/(scale*scale) factor, but there is a difference in\n",
    "  // notation; 'scale' here corresponds to 'alpha' in the paper, and\n",
    "  // 'update_speed' corresponds to 'nu' in the paper.\n",
    "  BaseFloat alpha = update_speed / (scale * scale);\n",
    "\n",
    "  // At this point, the matrix P contains what, in the math, would be Q =\n",
    "  // P-scale^2*I.  The derivative of the objective function w.r.t. an element q(i,j)\n",
    "  // of Q is now equal to -2*alpha*q(i,j), i.e. we could write q_deriv(i,j)\n",
    "  // = -2*alpha*q(i,j) This is also the derivative of the objective function\n",
    "  // w.r.t. p(i,j): i.e. p_deriv(i,j) = -2*alpha*q(i,j).\n",
    "  // Suppose we have define this matrix as 'P_deriv'.\n",
    "  // The derivative of the objective w.r.t M equals\n",
    "  // 2 * P_deriv * M, which equals -4*alpha*(P-scale^2*I)*M.\n",
    "  // (Currently the matrix P contains what, in the math, is P-scale^2*I).\n",
    "  M_update.AddMatMat(-4.0 * alpha, P, kNoTrans, *M, kNoTrans, 0.0);\n",
    "  M->AddMat(1.0, M_update);\n",
    "}\n",
    "\n",
    "/**\n",
    "   This function, to be called after processing every minibatch, is responsible\n",
    "   for enforcing the orthogonality constraint for any components of type\n",
    "   LinearComponent or inheriting from AffineComponent that have the\n",
    "   \"orthonormal_constraint\" value set.\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 128, 368]          36,736\n",
      "       BatchNorm1d-2             [-1, 128, 368]             256\n",
      "              ReLU-3             [-1, 128, 368]               0\n",
      "            Conv1d-4              [-1, 64, 368]           8,256\n",
      "       BatchNorm1d-5              [-1, 64, 368]             128\n",
      "            Conv1d-6             [-1, 128, 368]           1,792\n",
      "            Conv1d-7              [-1, 64, 368]           8,256\n",
      " AdaptiveAvgPool1d-8                [-1, 64, 1]               0\n",
      "            Linear-9                    [-1, 4]             256\n",
      "             ReLU-10                    [-1, 4]               0\n",
      "           Linear-11                   [-1, 64]             256\n",
      "          Sigmoid-12                   [-1, 64]               0\n",
      "          SEBlock-13              [-1, 64, 368]               0\n",
      "          TCSConv-14              [-1, 64, 368]               0\n",
      "      BatchNorm1d-15              [-1, 64, 368]             128\n",
      "             ReLU-16              [-1, 64, 368]               0\n",
      "          Dropout-17              [-1, 64, 368]               0\n",
      "         SubBlock-18              [-1, 64, 368]               0\n",
      "           Conv1d-19              [-1, 64, 368]             896\n",
      "           Conv1d-20              [-1, 64, 368]           4,160\n",
      "AdaptiveAvgPool1d-21                [-1, 64, 1]               0\n",
      "           Linear-22                    [-1, 4]             256\n",
      "             ReLU-23                    [-1, 4]               0\n",
      "           Linear-24                   [-1, 64]             256\n",
      "          Sigmoid-25                   [-1, 64]               0\n",
      "          SEBlock-26              [-1, 64, 368]               0\n",
      "          TCSConv-27              [-1, 64, 368]               0\n",
      "      BatchNorm1d-28              [-1, 64, 368]             128\n",
      "             ReLU-29              [-1, 64, 368]               0\n",
      "          Dropout-30              [-1, 64, 368]               0\n",
      "         SubBlock-31              [-1, 64, 368]               0\n",
      "        MainBlock-32              [-1, 64, 368]               0\n",
      "           Conv1d-33              [-1, 64, 368]           4,160\n",
      "      BatchNorm1d-34              [-1, 64, 368]             128\n",
      "           Conv1d-35              [-1, 64, 368]           1,024\n",
      "           Conv1d-36              [-1, 64, 368]           4,160\n",
      "AdaptiveAvgPool1d-37                [-1, 64, 1]               0\n",
      "           Linear-38                    [-1, 4]             256\n",
      "             ReLU-39                    [-1, 4]               0\n",
      "           Linear-40                   [-1, 64]             256\n",
      "          Sigmoid-41                   [-1, 64]               0\n",
      "          SEBlock-42              [-1, 64, 368]               0\n",
      "          TCSConv-43              [-1, 64, 368]               0\n",
      "      BatchNorm1d-44              [-1, 64, 368]             128\n",
      "             ReLU-45              [-1, 64, 368]               0\n",
      "          Dropout-46              [-1, 64, 368]               0\n",
      "         SubBlock-47              [-1, 64, 368]               0\n",
      "           Conv1d-48              [-1, 64, 368]           1,024\n",
      "           Conv1d-49              [-1, 64, 368]           4,160\n",
      "AdaptiveAvgPool1d-50                [-1, 64, 1]               0\n",
      "           Linear-51                    [-1, 4]             256\n",
      "             ReLU-52                    [-1, 4]               0\n",
      "           Linear-53                   [-1, 64]             256\n",
      "          Sigmoid-54                   [-1, 64]               0\n",
      "          SEBlock-55              [-1, 64, 368]               0\n",
      "          TCSConv-56              [-1, 64, 368]               0\n",
      "      BatchNorm1d-57              [-1, 64, 368]             128\n",
      "             ReLU-58              [-1, 64, 368]               0\n",
      "          Dropout-59              [-1, 64, 368]               0\n",
      "         SubBlock-60              [-1, 64, 368]               0\n",
      "        MainBlock-61              [-1, 64, 368]               0\n",
      "           Conv1d-62              [-1, 64, 368]           4,160\n",
      "      BatchNorm1d-63              [-1, 64, 368]             128\n",
      "           Conv1d-64              [-1, 64, 368]           1,152\n",
      "           Conv1d-65              [-1, 64, 368]           4,160\n",
      "AdaptiveAvgPool1d-66                [-1, 64, 1]               0\n",
      "           Linear-67                    [-1, 4]             256\n",
      "             ReLU-68                    [-1, 4]               0\n",
      "           Linear-69                   [-1, 64]             256\n",
      "          Sigmoid-70                   [-1, 64]               0\n",
      "          SEBlock-71              [-1, 64, 368]               0\n",
      "          TCSConv-72              [-1, 64, 368]               0\n",
      "      BatchNorm1d-73              [-1, 64, 368]             128\n",
      "             ReLU-74              [-1, 64, 368]               0\n",
      "          Dropout-75              [-1, 64, 368]               0\n",
      "         SubBlock-76              [-1, 64, 368]               0\n",
      "           Conv1d-77              [-1, 64, 368]           1,152\n",
      "           Conv1d-78              [-1, 64, 368]           4,160\n",
      "AdaptiveAvgPool1d-79                [-1, 64, 1]               0\n",
      "           Linear-80                    [-1, 4]             256\n",
      "             ReLU-81                    [-1, 4]               0\n",
      "           Linear-82                   [-1, 64]             256\n",
      "          Sigmoid-83                   [-1, 64]               0\n",
      "          SEBlock-84              [-1, 64, 368]               0\n",
      "          TCSConv-85              [-1, 64, 368]               0\n",
      "      BatchNorm1d-86              [-1, 64, 368]             128\n",
      "             ReLU-87              [-1, 64, 368]               0\n",
      "          Dropout-88              [-1, 64, 368]               0\n",
      "         SubBlock-89              [-1, 64, 368]               0\n",
      "        MainBlock-90              [-1, 64, 368]               0\n",
      "           Conv1d-91             [-1, 128, 312]         237,696\n",
      "      BatchNorm1d-92             [-1, 128, 312]             256\n",
      "             ReLU-93             [-1, 128, 312]               0\n",
      "           Conv1d-94             [-1, 128, 312]          16,512\n",
      "      BatchNorm1d-95             [-1, 128, 312]             256\n",
      "             ReLU-96             [-1, 128, 312]               0\n",
      "           Conv1d-97             [-1, 128, 312]          16,512\n",
      "      BatchNorm1d-98             [-1, 128, 312]             256\n",
      "             ReLU-99             [-1, 128, 312]               0\n",
      "          Conv1d-100              [-1, 17, 312]           2,193\n",
      "AdaptiveAvgPool1d-101                [-1, 17, 1]               0\n",
      "================================================================\n",
      "Total params: 367,569\n",
      "Trainable params: 367,569\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.07\n",
      "Forward/backward pass size (MB): 14.29\n",
      "Params size (MB): 1.40\n",
      "Estimated Total Size (MB): 15.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = XEmbedding(in_dim=30) #we will try 0 (HLTCOE) or 40 MFCC (CLSP-MIT) for 16 KHz\n",
    "summary(model, input_size=(667, 30))  # Adjust input_size to     match (batch_size, input_dim),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using.. cpu\n",
      "Epoch [1/100] Train Loss: 1.5863, Train Acc: 0.5841 Val Loss: 1.0931, Val Acc: 0.7338\n",
      "--------------------\n",
      "Epoch [2/100] Train Loss: 0.7238, Train Acc: 0.8242 Val Loss: 0.6187, Val Acc: 0.8331\n",
      "--------------------\n",
      "Epoch [3/100] Train Loss: 0.4831, Train Acc: 0.8760 Val Loss: 0.6623, Val Acc: 0.8075\n",
      "--------------------\n",
      "Epoch [4/100] Train Loss: 0.3820, Train Acc: 0.8974 Val Loss: 0.5425, Val Acc: 0.8466\n",
      "--------------------\n",
      "Epoch [5/100] Train Loss: 0.3376, Train Acc: 0.9008 Val Loss: 0.3562, Val Acc: 0.8932\n",
      "--------------------\n",
      "Epoch [6/100] Train Loss: 0.2881, Train Acc: 0.9146 Val Loss: 0.4443, Val Acc: 0.8617\n",
      "--------------------\n",
      "Epoch [7/100] Train Loss: 0.2390, Train Acc: 0.9295 Val Loss: 0.3815, Val Acc: 0.8887\n",
      "--------------------\n",
      "Epoch [8/100] Train Loss: 0.2284, Train Acc: 0.9303 Val Loss: 0.3172, Val Acc: 0.8962\n",
      "--------------------\n",
      "Epoch [9/100] Train Loss: 0.2164, Train Acc: 0.9365 Val Loss: 0.3615, Val Acc: 0.8707\n",
      "--------------------\n",
      "Epoch [10/100] Train Loss: 0.2078, Train Acc: 0.9358 Val Loss: 0.3576, Val Acc: 0.8932\n",
      "--------------------\n",
      "Epoch [11/100] Train Loss: 0.1762, Train Acc: 0.9472 Val Loss: 0.2983, Val Acc: 0.9023\n",
      "--------------------\n",
      "Epoch [12/100] Train Loss: 0.1823, Train Acc: 0.9429 Val Loss: 0.5166, Val Acc: 0.8541\n",
      "--------------------\n",
      "Epoch [13/100] Train Loss: 0.1648, Train Acc: 0.9460 Val Loss: 0.2277, Val Acc: 0.9173\n",
      "--------------------\n",
      "Epoch [14/100] Train Loss: 0.1493, Train Acc: 0.9547 Val Loss: 0.4068, Val Acc: 0.8752\n",
      "--------------------\n",
      "Epoch [15/100] Train Loss: 0.1466, Train Acc: 0.9512 Val Loss: 0.3115, Val Acc: 0.9038\n",
      "--------------------\n",
      "Epoch [16/100] Train Loss: 0.1323, Train Acc: 0.9562 Val Loss: 0.2289, Val Acc: 0.9188\n",
      "--------------------\n",
      "Epoch [17/100] Train Loss: 0.1255, Train Acc: 0.9596 Val Loss: 0.3089, Val Acc: 0.9128\n",
      "--------------------\n",
      "Epoch [18/100] Train Loss: 0.1181, Train Acc: 0.9627 Val Loss: 0.4422, Val Acc: 0.8827\n",
      "--------------------\n",
      "Epoch [19/100] Train Loss: 0.1239, Train Acc: 0.9589 Val Loss: 0.3627, Val Acc: 0.9068\n",
      "--------------------\n",
      "Epoch [20/100] Train Loss: 0.0785, Train Acc: 0.9761 Val Loss: 0.1387, Val Acc: 0.9534\n",
      "--------------------\n",
      "Epoch [21/100] Train Loss: 0.0665, Train Acc: 0.9793 Val Loss: 0.1768, Val Acc: 0.9398\n",
      "--------------------\n",
      "Epoch [22/100] Train Loss: 0.0668, Train Acc: 0.9794 Val Loss: 0.1424, Val Acc: 0.9519\n",
      "--------------------\n",
      "Epoch [23/100] Train Loss: 0.0672, Train Acc: 0.9788 Val Loss: 0.1434, Val Acc: 0.9474\n",
      "--------------------\n",
      "Epoch [24/100] Train Loss: 0.0592, Train Acc: 0.9830 Val Loss: 0.1504, Val Acc: 0.9489\n",
      "--------------------\n",
      "Epoch [25/100] Train Loss: 0.0550, Train Acc: 0.9841 Val Loss: 0.1516, Val Acc: 0.9594\n",
      "--------------------\n",
      "Epoch [26/100] Train Loss: 0.0589, Train Acc: 0.9808 Val Loss: 0.1616, Val Acc: 0.9383\n",
      "--------------------\n",
      "Epoch [27/100] Train Loss: 0.0413, Train Acc: 0.9880 Val Loss: 0.1196, Val Acc: 0.9549\n",
      "--------------------\n",
      "Epoch [28/100] Train Loss: 0.0398, Train Acc: 0.9896 Val Loss: 0.1239, Val Acc: 0.9579\n",
      "--------------------\n",
      "Epoch [29/100] Train Loss: 0.0333, Train Acc: 0.9906 Val Loss: 0.1311, Val Acc: 0.9504\n",
      "--------------------\n",
      "Epoch [30/100] Train Loss: 0.0311, Train Acc: 0.9908 Val Loss: 0.1300, Val Acc: 0.9549\n",
      "--------------------\n",
      "Epoch [31/100] Train Loss: 0.0347, Train Acc: 0.9920 Val Loss: 0.1134, Val Acc: 0.9639\n",
      "--------------------\n",
      "Epoch [32/100] Train Loss: 0.0313, Train Acc: 0.9921 Val Loss: 0.1378, Val Acc: 0.9594\n",
      "--------------------\n",
      "Epoch [33/100] Train Loss: 0.0261, Train Acc: 0.9938 Val Loss: 0.1011, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [34/100] Train Loss: 0.0294, Train Acc: 0.9916 Val Loss: 0.1264, Val Acc: 0.9579\n",
      "--------------------\n",
      "Epoch [35/100] Train Loss: 0.0329, Train Acc: 0.9901 Val Loss: 0.1190, Val Acc: 0.9609\n",
      "--------------------\n",
      "Epoch [36/100] Train Loss: 0.0277, Train Acc: 0.9923 Val Loss: 0.1170, Val Acc: 0.9624\n",
      "--------------------\n",
      "Epoch [37/100] Train Loss: 0.0271, Train Acc: 0.9920 Val Loss: 0.1149, Val Acc: 0.9564\n",
      "--------------------\n",
      "Epoch [38/100] Train Loss: 0.0281, Train Acc: 0.9918 Val Loss: 0.1074, Val Acc: 0.9639\n",
      "--------------------\n",
      "Epoch [39/100] Train Loss: 0.0314, Train Acc: 0.9906 Val Loss: 0.1272, Val Acc: 0.9564\n",
      "--------------------\n",
      "Epoch [40/100] Train Loss: 0.0178, Train Acc: 0.9967 Val Loss: 0.1055, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [41/100] Train Loss: 0.0204, Train Acc: 0.9947 Val Loss: 0.0941, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [42/100] Train Loss: 0.0185, Train Acc: 0.9955 Val Loss: 0.0928, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [43/100] Train Loss: 0.0190, Train Acc: 0.9948 Val Loss: 0.0994, Val Acc: 0.9654\n",
      "--------------------\n",
      "Epoch [44/100] Train Loss: 0.0208, Train Acc: 0.9938 Val Loss: 0.1128, Val Acc: 0.9639\n",
      "--------------------\n",
      "Epoch [45/100] Train Loss: 0.0193, Train Acc: 0.9943 Val Loss: 0.0803, Val Acc: 0.9729\n",
      "--------------------\n",
      "Epoch [46/100] Train Loss: 0.0165, Train Acc: 0.9958 Val Loss: 0.0826, Val Acc: 0.9729\n",
      "--------------------\n",
      "Epoch [47/100] Train Loss: 0.0218, Train Acc: 0.9937 Val Loss: 0.0951, Val Acc: 0.9654\n",
      "--------------------\n",
      "Epoch [48/100] Train Loss: 0.0149, Train Acc: 0.9963 Val Loss: 0.0751, Val Acc: 0.9759\n",
      "--------------------\n",
      "Epoch [49/100] Train Loss: 0.0181, Train Acc: 0.9952 Val Loss: 0.0945, Val Acc: 0.9654\n",
      "--------------------\n",
      "Epoch [50/100] Train Loss: 0.0153, Train Acc: 0.9967 Val Loss: 0.0947, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [51/100] Train Loss: 0.0163, Train Acc: 0.9957 Val Loss: 0.0929, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [52/100] Train Loss: 0.0172, Train Acc: 0.9958 Val Loss: 0.0961, Val Acc: 0.9654\n",
      "--------------------\n",
      "Epoch [53/100] Train Loss: 0.0164, Train Acc: 0.9972 Val Loss: 0.0922, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [54/100] Train Loss: 0.0164, Train Acc: 0.9967 Val Loss: 0.0794, Val Acc: 0.9744\n",
      "--------------------\n",
      "Epoch [55/100] Train Loss: 0.0152, Train Acc: 0.9967 Val Loss: 0.0833, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [56/100] Train Loss: 0.0103, Train Acc: 0.9980 Val Loss: 0.0827, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [57/100] Train Loss: 0.0107, Train Acc: 0.9980 Val Loss: 0.0866, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [58/100] Train Loss: 0.0137, Train Acc: 0.9957 Val Loss: 0.0869, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [59/100] Train Loss: 0.0134, Train Acc: 0.9967 Val Loss: 0.0853, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [60/100] Train Loss: 0.0115, Train Acc: 0.9978 Val Loss: 0.0831, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [61/100] Train Loss: 0.0123, Train Acc: 0.9978 Val Loss: 0.0830, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [62/100] Train Loss: 0.0092, Train Acc: 0.9987 Val Loss: 0.0855, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [63/100] Train Loss: 0.0096, Train Acc: 0.9983 Val Loss: 0.0886, Val Acc: 0.9624\n",
      "--------------------\n",
      "Epoch [64/100] Train Loss: 0.0098, Train Acc: 0.9977 Val Loss: 0.0834, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [65/100] Train Loss: 0.0127, Train Acc: 0.9970 Val Loss: 0.0868, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [66/100] Train Loss: 0.0124, Train Acc: 0.9980 Val Loss: 0.0847, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [67/100] Train Loss: 0.0118, Train Acc: 0.9965 Val Loss: 0.0868, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [68/100] Train Loss: 0.0086, Train Acc: 0.9992 Val Loss: 0.0852, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [69/100] Train Loss: 0.0083, Train Acc: 0.9993 Val Loss: 0.0870, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [70/100] Train Loss: 0.0103, Train Acc: 0.9980 Val Loss: 0.0834, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [71/100] Train Loss: 0.0125, Train Acc: 0.9972 Val Loss: 0.0834, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [72/100] Train Loss: 0.0124, Train Acc: 0.9972 Val Loss: 0.0766, Val Acc: 0.9744\n",
      "--------------------\n",
      "Epoch [73/100] Train Loss: 0.0089, Train Acc: 0.9985 Val Loss: 0.0790, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [74/100] Train Loss: 0.0082, Train Acc: 0.9985 Val Loss: 0.0868, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [75/100] Train Loss: 0.0086, Train Acc: 0.9987 Val Loss: 0.0865, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [76/100] Train Loss: 0.0113, Train Acc: 0.9973 Val Loss: 0.0820, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [77/100] Train Loss: 0.0086, Train Acc: 0.9983 Val Loss: 0.0820, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [78/100] Train Loss: 0.0089, Train Acc: 0.9988 Val Loss: 0.0871, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [79/100] Train Loss: 0.0095, Train Acc: 0.9987 Val Loss: 0.0835, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [80/100] Train Loss: 0.0102, Train Acc: 0.9982 Val Loss: 0.0866, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [81/100] Train Loss: 0.0109, Train Acc: 0.9977 Val Loss: 0.0829, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [82/100] Train Loss: 0.0088, Train Acc: 0.9985 Val Loss: 0.0808, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [83/100] Train Loss: 0.0123, Train Acc: 0.9965 Val Loss: 0.0851, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [84/100] Train Loss: 0.0110, Train Acc: 0.9980 Val Loss: 0.0853, Val Acc: 0.9729\n",
      "--------------------\n",
      "Epoch [85/100] Train Loss: 0.0090, Train Acc: 0.9985 Val Loss: 0.0813, Val Acc: 0.9744\n",
      "--------------------\n",
      "Epoch [86/100] Train Loss: 0.0088, Train Acc: 0.9983 Val Loss: 0.0813, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [87/100] Train Loss: 0.0107, Train Acc: 0.9977 Val Loss: 0.0829, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [88/100] Train Loss: 0.0080, Train Acc: 0.9987 Val Loss: 0.0826, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [89/100] Train Loss: 0.0112, Train Acc: 0.9968 Val Loss: 0.0821, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [90/100] Train Loss: 0.0097, Train Acc: 0.9980 Val Loss: 0.0830, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [91/100] Train Loss: 0.0090, Train Acc: 0.9983 Val Loss: 0.0850, Val Acc: 0.9684\n",
      "--------------------\n",
      "Epoch [92/100] Train Loss: 0.0097, Train Acc: 0.9978 Val Loss: 0.0861, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [93/100] Train Loss: 0.0102, Train Acc: 0.9980 Val Loss: 0.0811, Val Acc: 0.9729\n",
      "--------------------\n",
      "Epoch [94/100] Train Loss: 0.0096, Train Acc: 0.9987 Val Loss: 0.0815, Val Acc: 0.9669\n",
      "--------------------\n",
      "Epoch [95/100] Train Loss: 0.0122, Train Acc: 0.9968 Val Loss: 0.0800, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [96/100] Train Loss: 0.0091, Train Acc: 0.9983 Val Loss: 0.0815, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [97/100] Train Loss: 0.0127, Train Acc: 0.9973 Val Loss: 0.0809, Val Acc: 0.9714\n",
      "--------------------\n",
      "Epoch [98/100] Train Loss: 0.0092, Train Acc: 0.9985 Val Loss: 0.0788, Val Acc: 0.9729\n",
      "--------------------\n",
      "Epoch [99/100] Train Loss: 0.0089, Train Acc: 0.9982 Val Loss: 0.0859, Val Acc: 0.9699\n",
      "--------------------\n",
      "Epoch [100/100] Train Loss: 0.0102, Train Acc: 0.9975 Val Loss: 0.0791, Val Acc: 0.9714\n",
      "--------------------\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=5, grad_clip=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using..\",device)\n",
    "    model.to(device)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
    "   # scheduler = OneCycleLR(\n",
    "    #optimizer,\n",
    "  #  max_lr=0.01,  # Maximum learning rate\n",
    "   # steps_per_epoch=steps_per_epoch,\n",
    "  #  epochs=num_epochs,\n",
    "   # pct_start=0.3,  # Percentage of the cycle to increase learning rate\n",
    "   # anneal_strategy='cos',  # Annealing strategy ('linear' or 'cos')\n",
    "   # div_factor=25.0,  # Initial learning rate = max_lr / div_factor\n",
    "   # final_div_factor=1e4  # Minimum learning rate = initial_lr / final_div_factor\n",
    "#)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), grad_clip) #maybe\n",
    "\n",
    "            optimizer.step()\n",
    "            if (epoch+1)%4==0:\n",
    "                model.step_semi_orth() # The key method to constrain the first two convolutions, perform after every 4 SGD step\n",
    "                with open(\"orth_error_log.txt\", \"w\") as f:\n",
    "                    f.write(str(model.get_orth_errors())) # This returns the orth error of the constrained convs, useful for debugging\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'XEmbedding/XEmbedding_101.pt')\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "num_classes = len(train_dataset.label_encoder.classes_)\n",
    "model = XEmbedding()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading audio: 100%|██████████| 7/7 [00:00<00:00, 456.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL is using CPU for Inferencing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = OptimizedAudioDataset(csv_file=\"test.csv\",\n",
    "                                      audio_dir=\"test/Test\",\n",
    "                                      transform=optimized_audio_transform,\n",
    "                                      is_test=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=70, shuffle=False,\n",
    "                          collate_fn=custom_collate_fn, num_workers=0)\n",
    "                        \n",
    "def predict_test_data(model, test_loader):\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    print(\"MODEL is using CPU for Inferencing....\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs in tqdm(test_loader, desc=\"Inferencing\"):\n",
    "            inputs = inputs.to(device)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "test_predictions = predict_test_data(model_inference, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\Ahmed_ali_ajmi.wav</td>\n",
       "      <td>Ahmed_ali_ajmi.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\Alimahmoud.wav</td>\n",
       "      <td>Alimahmoud.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\Islem_sob7i.wav</td>\n",
       "      <td>Islem_sob7i.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\Yaser_aldousari.wav</td>\n",
       "      <td>Yaser_aldousari.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\Youssef_aidours.wav</td>\n",
       "      <td>Youssef_aidours.m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\phone_audio.wav</td>\n",
       "      <td>yasser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:\\CNN Project\\test\\Test\\surah_al_mulk-yasser_...</td>\n",
       "      <td>yasser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path                label\n",
       "0        C:\\CNN Project\\test\\Test\\Ahmed_ali_ajmi.wav   Ahmed_ali_ajmi.m4a\n",
       "1            C:\\CNN Project\\test\\Test\\Alimahmoud.wav       Alimahmoud.m4a\n",
       "2           C:\\CNN Project\\test\\Test\\Islem_sob7i.wav      Islem_sob7i.m4a\n",
       "3       C:\\CNN Project\\test\\Test\\Yaser_aldousari.wav  Yaser_aldousari.m4a\n",
       "4       C:\\CNN Project\\test\\Test\\Youssef_aidours.wav  Youssef_aidours.m4a\n",
       "5           C:\\CNN Project\\test\\Test\\phone_audio.wav               yasser\n",
       "6  C:\\CNN Project\\test\\Test\\surah_al_mulk-yasser_...               yasser"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv(\"test.csv\")\n",
    "test.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ahmed_alijmi', 'abd_albassit_abd_samad', 'abd_rahman_sodays',\n",
       "       'abd_albassit_abd_samad', 'abd_rahman_sodays', 'yasser_aldossari',\n",
       "       'yasser_aldossari'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label_encoder.inverse_transform(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
